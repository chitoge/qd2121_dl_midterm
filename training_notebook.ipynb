{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "collapsed_sections": [
        "0b1e0a43"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a8bfac9ffe745c8aa3aaf3d5fe4997b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_715ed9569a5b4bfa8604becb9c70fe85",
              "IPY_MODEL_d5c70a68132e43478020d7915e4eab7c",
              "IPY_MODEL_ff7b5be51b334296b8ee9f6af7b85b64"
            ],
            "layout": "IPY_MODEL_ea6c91c7fee64aa382180460b04b36f7"
          }
        },
        "715ed9569a5b4bfa8604becb9c70fe85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77484ac6430542439ab0785e694a9dda",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0a25516eb25f46d8ba26015acf9a84ab",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "d5c70a68132e43478020d7915e4eab7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652149e9ca484b3ca3f4ad6fdeb281f3",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1ceb6d64907459688246af80a48f158",
            "value": 4
          }
        },
        "ff7b5be51b334296b8ee9f6af7b85b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10168613745346d5ad440d22912a6f18",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8483c79c5592452a85e05efe2e641407",
            "value": "‚Äá4/4‚Äá[00:04&lt;00:00,‚Äá‚Äá1.01it/s]"
          }
        },
        "ea6c91c7fee64aa382180460b04b36f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77484ac6430542439ab0785e694a9dda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a25516eb25f46d8ba26015acf9a84ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "652149e9ca484b3ca3f4ad6fdeb281f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1ceb6d64907459688246af80a48f158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10168613745346d5ad440d22912a6f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8483c79c5592452a85e05efe2e641407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9e6fa288ab74bd1b07b5cd3f09143c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_baf3e4c3b0dc40aa87a958d6da7e4159",
              "IPY_MODEL_0f951a9cc0b048978dc86d23b8cf736f",
              "IPY_MODEL_529f751444a8402291d12c84faa3a550"
            ],
            "layout": "IPY_MODEL_d1066e44454d42faac0ef8f7c639ed54"
          }
        },
        "baf3e4c3b0dc40aa87a958d6da7e4159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229c1aca17ec43ca82c2d1961dc2d2d7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_806d639b44c4404493ba522db695ba5e",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "0f951a9cc0b048978dc86d23b8cf736f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af2c9c32d9c04dc481ca5070159d6bb9",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9af2491ac906478b8219c402c84b7eb2",
            "value": 4
          }
        },
        "529f751444a8402291d12c84faa3a550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ceb61ac9d6485abed480d30f9c7c7f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c83f43223c99411c84d8a3282c3a9bcc",
            "value": "‚Äá4/4‚Äá[00:04&lt;00:00,‚Äá‚Äá1.01it/s]"
          }
        },
        "d1066e44454d42faac0ef8f7c639ed54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229c1aca17ec43ca82c2d1961dc2d2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806d639b44c4404493ba522db695ba5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af2c9c32d9c04dc481ca5070159d6bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af2491ac906478b8219c402c84b7eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7ceb61ac9d6485abed480d30f9c7c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83f43223c99411c84d8a3282c3a9bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Notebook\n",
        "\n",
        "ECE-GY 7123 / CS-GY 6953 / Deep Learning - Fall '25 - Midterm Project\n",
        "\n",
        "**Team:** Spline Reticulator\n",
        "\n",
        "**Author/Member:** Thanh Do (qd2121@nyu.edu)"
      ],
      "metadata": {
        "id": "jngXVlfmqT9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Necessary Libraries\n",
        "\n",
        "Instead of the original installation step in the starter notebook, I used the fix from Aryan to install dependencies from https://campuswire.com/c/GF164CBA5/feed/70:\n"
      ],
      "metadata": {
        "id": "h6H4hQVSqblY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xStnwtpOqK0e",
        "outputId": "f5ee5e0e-88aa-4256-f751-0789ec0fcd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: xformers==0.0.32.post2 in /usr/local/lib/python3.12/dist-packages (0.0.32.post2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (25.1.1)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.12/dist-packages (2025.10.13)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
            "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (1.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.13.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.10.12)\n",
            "Requirement already satisfied: transformers==4.56.2 in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2025.10.5)\n",
            "Requirement already satisfied: trl==0.22.2 in /usr/local/lib/python3.12/dist-packages (0.22.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining Model and Constants\n",
        "\n",
        "In this step, we will load the base model as required by the competition. I also defined a few global constants here so that we can easily modify the training hyperparameters as required."
      ],
      "metadata": {
        "id": "TkuYDaVuravN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # 1024 is not enough for some questions\n",
        "dtype = None  # This will auto-detect the best data type for your GPU\n",
        "# We have a lot of VRAM on the A100 so this might save some quantization time\n",
        "load_in_4bit = False\n",
        "\n",
        "# Some constants\n",
        "global_seed = 313337\n",
        "global_inference_prompt = \"\"\"You are an expert mathematician and a meticulous verifier.\n",
        "Your task is to evaluate a proposed solution to a math problem and determine if it's correct or not.\n",
        "Carefully read the Question and the Solution. Determine if the Solution is a correct reasoning process to solve the Question.\n",
        "Your response should be 'True' if the solution is correct, otherwise 'False'.\n",
        "Below is the Question and Solution.\n",
        "Question:\n",
        "{}\n",
        "Solution:\n",
        "{}\n",
        "Output:\n",
        "\"\"\"\n",
        "global_training_prompt = global_inference_prompt + \"{}\" # include the correct answer\n",
        "global_checkpoint_path = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint_less_naive_8k5_steps\"\n",
        "global_max_steps = 8500 # Just right to use all of my remaining compute units :(\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "# Note: We use the base model, not a 4-bit pre-quantized one,\n",
        "# to ensure we start from the official weights.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\", # Competition-approved model\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "8a8bfac9ffe745c8aa3aaf3d5fe4997b",
            "715ed9569a5b4bfa8604becb9c70fe85",
            "d5c70a68132e43478020d7915e4eab7c",
            "ff7b5be51b334296b8ee9f6af7b85b64",
            "ea6c91c7fee64aa382180460b04b36f7",
            "77484ac6430542439ab0785e694a9dda",
            "0a25516eb25f46d8ba26015acf9a84ab",
            "652149e9ca484b3ca3f4ad6fdeb281f3",
            "a1ceb6d64907459688246af80a48f158",
            "10168613745346d5ad440d22912a6f18",
            "8483c79c5592452a85e05efe2e641407"
          ]
        },
        "id": "URSw7qlhqlgB",
        "outputId": "61c75f50-f500-4750-d8d7-36934d15f3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a8bfac9ffe745c8aa3aaf3d5fe4997b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare the Dataset\n",
        "\n",
        "Like the starter notebook, the process here involves three parts:\n",
        "\n",
        "1.  **Loading**: We'll load the official competition dataset from Hugging Face.\n",
        "2.  **Splitting**: The full dataset is massive. We will shuffle the train split into two parts: **500 samples for validation** and **the rest for training**. The train split will be shuffled using our global seed.\n",
        "3.  **Prompting**: We will format each data sample using the globally-defined training prompt.\n",
        "\n"
      ],
      "metadata": {
        "id": "sRznzEwL3W-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full training dataset\n",
        "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "\n",
        "# Shuffle the dataset for randomness and create our smaller splits\n",
        "shuffled_dataset = full_dataset.shuffle(seed=global_seed)\n",
        "n = len(full_dataset)\n",
        "# Takes the last 500 samples for internal validation\n",
        "n_validation = 500\n",
        "train_dataset = shuffled_dataset.select(range(n - n_validation))\n",
        "validation_dataset = shuffled_dataset.select(range(n - n_validation, n))"
      ],
      "metadata": {
        "id": "etaDwWGN3X7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I defined some common data clean up implementations as detailed in Section 5.3 in my midterm report. This is supposed to be used for both training and inference."
      ],
      "metadata": {
        "id": "MZG3AsMZUXv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def round_floats_in_text(text: str, n_digits: int = 4) -> str:\n",
        "    \"\"\"\n",
        "    Finds all floating-point numbers in a string and rounds them\n",
        "    to a specified number of decimal places.\n",
        "    \"\"\"\n",
        "\n",
        "    # Matches an optional sign, digits, a decimal point, and more digits.\n",
        "    float_pattern = re.compile(r\"[-]?\\d+\\.\\d+\")\n",
        "\n",
        "    # m.group(0) is the matched text (e.g., \"0.666666667\")\n",
        "    def replacer(match):\n",
        "        # Convert the matched string to a float\n",
        "        number = float(match.group(0))\n",
        "        # Round it to 'n_digits'\n",
        "        rounded_number = round(number, n_digits)\n",
        "        # Convert it back to a string\n",
        "        return str(rounded_number)\n",
        "\n",
        "    return float_pattern.sub(replacer, text)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "  return round_floats_in_text(text)"
      ],
      "metadata": {
        "id": "d2_f4JgJUVww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare training:"
      ],
      "metadata": {
        "id": "uJCEJoegUghM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The instructional prompt template for training\n",
        "training_prompt = global_training_prompt\n",
        "\n",
        "# We must add an End Of Sequence (EOS) token to tell the model when a completion is finished.\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# This function formats our data samples into the prompt template.\n",
        "def formatting_prompts_func(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    solutions = examples[\"solution\"]\n",
        "    outputs = examples[\"is_correct\"]\n",
        "    texts = []\n",
        "    for question, solution, output in zip(questions, solutions, outputs):\n",
        "        # Format the prompt and add the EOS token\n",
        "        text = training_prompt.format(question, str(solution), str(output))\n",
        "        # Clean data\n",
        "        text = clean_text(text)\n",
        "        text += EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# Apply the formatting function to our training dataset\n",
        "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "i5cL3djv3bRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Configure LoRA and Set Up the Trainer**\n"
      ],
      "metadata": {
        "id": "K8Fs1qmn37-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LoRA Configuration**\n",
        "\n",
        "Instead of training the entire model (which has billions of parameters), we'll use a technique called **Lo**w-**R**ank **A**daptation (LoRA). üéõÔ∏è\n",
        "\n",
        "Think of it like this: rather than rewriting an entire textbook, we're just adding small, efficient \"sticky notes\" (the LoRA adapters) to update the model's knowledge. This is much faster and requires significantly less memory.\n"
      ],
      "metadata": {
        "id": "nZBmfB3cVv7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Seems to be a decent rank value\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 64, # A common practice is to set alpha = 2 * r\n",
        "    lora_dropout = 0, # Unsloth will complain about perf if not zero\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = global_seed,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEaRjozB3tz8",
        "outputId": "f1ab411a-ea36-435b-eeb0-9eb6628c281f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.10.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **SFTTrainer Setup**\n",
        "\n",
        "Now we'll set up the `SFTTrainer` (Supervised Fine-tuning Trainer). This is the main tool from the `trl` library that will handle the entire training loop for us. We'll give it our model, tokenizer, dataset, and a set of training instructions, such as the batch size and number of epochs.\n",
        "\n",
        "Here I also modified some arguments in order to utilize our A100 runtime environment better."
      ],
      "metadata": {
        "id": "HCHdotc14DgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = TrainingArguments(\n",
        "        num_train_epochs=1,\n",
        "        # Use all the VRAMs!\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        # Also try use all the RAMs. This does not seem to be it though\n",
        "        dataloader_num_workers = 8,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = global_max_steps,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 10,\n",
        "        # Since we're not using quantization, just use PyTorch's 16-bit impl\n",
        "        optim = \"adamw_torch\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = global_seed,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "yVZHQ4y74BCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Start Training\\!**\n",
        "\n",
        "Now, we'll call the `train()` function on our `trainer` object. This will kick off the fine-tuning process.\n"
      ],
      "metadata": {
        "id": "GTHBzKeM4zF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(resume_from_checkpoint = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YVrNhZ4y4zsK",
        "outputId": "b5b4fe94-d69b-468e-9278-9f5a8310d60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 999,500 | Num Epochs = 1 | Total steps = 8,500\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8500' max='8500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8500/8500 1:31:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7510</td>\n",
              "      <td>0.373600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7520</td>\n",
              "      <td>0.363800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7530</td>\n",
              "      <td>0.352500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7540</td>\n",
              "      <td>0.374700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7550</td>\n",
              "      <td>0.362400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7560</td>\n",
              "      <td>0.374600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7570</td>\n",
              "      <td>0.336800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7580</td>\n",
              "      <td>0.355300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7590</td>\n",
              "      <td>0.351100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.353700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7610</td>\n",
              "      <td>0.332800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7620</td>\n",
              "      <td>0.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7630</td>\n",
              "      <td>0.346500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7640</td>\n",
              "      <td>0.358100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7650</td>\n",
              "      <td>0.355900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7660</td>\n",
              "      <td>0.349600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7670</td>\n",
              "      <td>0.370300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7680</td>\n",
              "      <td>0.342500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7690</td>\n",
              "      <td>0.324000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>0.329000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7710</td>\n",
              "      <td>0.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7720</td>\n",
              "      <td>0.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7730</td>\n",
              "      <td>0.362400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7740</td>\n",
              "      <td>0.347100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7750</td>\n",
              "      <td>0.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7760</td>\n",
              "      <td>0.354100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7770</td>\n",
              "      <td>0.366600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7780</td>\n",
              "      <td>0.369600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7790</td>\n",
              "      <td>0.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>0.335400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7810</td>\n",
              "      <td>0.359500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7820</td>\n",
              "      <td>0.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7830</td>\n",
              "      <td>0.362900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7840</td>\n",
              "      <td>0.328300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7850</td>\n",
              "      <td>0.348300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7860</td>\n",
              "      <td>0.339800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7870</td>\n",
              "      <td>0.339500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7880</td>\n",
              "      <td>0.352200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7890</td>\n",
              "      <td>0.396800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>0.335100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7910</td>\n",
              "      <td>0.368900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7920</td>\n",
              "      <td>0.358700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7930</td>\n",
              "      <td>0.350700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7940</td>\n",
              "      <td>0.365900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7950</td>\n",
              "      <td>0.342600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7960</td>\n",
              "      <td>0.362500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7970</td>\n",
              "      <td>0.367600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7980</td>\n",
              "      <td>0.346600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7990</td>\n",
              "      <td>0.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.348500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8010</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8020</td>\n",
              "      <td>0.346300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8030</td>\n",
              "      <td>0.343700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8040</td>\n",
              "      <td>0.333900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8050</td>\n",
              "      <td>0.345900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8060</td>\n",
              "      <td>0.379200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8070</td>\n",
              "      <td>0.359200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8080</td>\n",
              "      <td>0.347400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8090</td>\n",
              "      <td>0.403300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>0.354700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8110</td>\n",
              "      <td>0.350800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8120</td>\n",
              "      <td>0.366300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8130</td>\n",
              "      <td>0.349500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8140</td>\n",
              "      <td>0.351000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8150</td>\n",
              "      <td>0.344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8160</td>\n",
              "      <td>0.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8170</td>\n",
              "      <td>0.365500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8180</td>\n",
              "      <td>0.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8190</td>\n",
              "      <td>0.346500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>0.340300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8210</td>\n",
              "      <td>0.352300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8220</td>\n",
              "      <td>0.342400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8230</td>\n",
              "      <td>0.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8240</td>\n",
              "      <td>0.365300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8250</td>\n",
              "      <td>0.342300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8260</td>\n",
              "      <td>0.351800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8270</td>\n",
              "      <td>0.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8280</td>\n",
              "      <td>0.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8290</td>\n",
              "      <td>0.379200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>0.337400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8310</td>\n",
              "      <td>0.325900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8320</td>\n",
              "      <td>0.361800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8330</td>\n",
              "      <td>0.361100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8340</td>\n",
              "      <td>0.357500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8350</td>\n",
              "      <td>0.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8360</td>\n",
              "      <td>0.340300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8370</td>\n",
              "      <td>0.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8380</td>\n",
              "      <td>0.370200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8390</td>\n",
              "      <td>0.343300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8410</td>\n",
              "      <td>0.373200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8420</td>\n",
              "      <td>0.381000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8430</td>\n",
              "      <td>0.374200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8440</td>\n",
              "      <td>0.352300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8450</td>\n",
              "      <td>0.344800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8460</td>\n",
              "      <td>0.330500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8470</td>\n",
              "      <td>0.371200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8480</td>\n",
              "      <td>0.354100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8490</td>\n",
              "      <td>0.375800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.353300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8500, training_loss=0.041673962957718795, metrics={'train_runtime': 5513.8415, 'train_samples_per_second': 49.33, 'train_steps_per_second': 1.542, 'total_flos': 9.016436772337877e+18, 'train_loss': 0.041673962957718795, 'epoch': 0.2721306226988955})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Step 6: Inference and Evaluation**\n",
        "\n",
        "Now that our model is trained, we need to test it on our validation set. We'll use a slightly different prompt for inference‚Äîone where we leave the `Output:` section blank for the model to complete.\n",
        "\n",
        "Here, we'll run the model over our internal validation set to obtain a \"validation accuracy\" value, which we can use for evaluating our current approach."
      ],
      "metadata": {
        "id": "8LfJfk5gIyIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Create the prompt template for inference (no answer included)\n",
        "inference_prompt = global_inference_prompt\n",
        "\n",
        "# Evaluate accuracy on our validation set\n",
        "num_samples = len(validation_dataset)\n",
        "count_correct = 0\n",
        "for i in range(num_samples):\n",
        "  example = validation_dataset[i]\n",
        "  question = example[\"question\"]\n",
        "  solution = example[\"solution\"]\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "      inference_prompt.format(question, str(solution))\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 8, use_cache = True)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  prediction: str = response[0].split(\"Output:\\n\")[1]\n",
        "  if prediction.startswith(str(example[\"is_correct\"])):\n",
        "    count_correct += 1\n",
        "print(\"Validation Accuracy =\", count_correct / num_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agvQR_Ku5wWY",
        "outputId": "6f116314-2a01-4014-856e-e006d1cb39f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy = 0.892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Save the Model to Drive & Reload**\n"
      ],
      "metadata": {
        "id": "Ehz1Uly-JV-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Save the model checkpoint to Google Drive, load the model from the checkpoint, and generate the final submission CSV file."
      ],
      "metadata": {
        "id": "yurFNmwSVXtz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b1e0a43"
      },
      "source": [
        "### Mount google drive\n",
        "\n",
        "#### Subtask:\n",
        "Mount Google Drive to save the model checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8ab404"
      },
      "source": [
        "**Reasoning**:\n",
        "Mount Google Drive to save the model checkpoint.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e020e6b",
        "outputId": "c47858f4-7007-49cd-87a1-8d5ac2efdbe9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c28a7dd"
      },
      "source": [
        "### Save model checkpoint\n",
        "\n",
        "#### Subtask:\n",
        "Save the trained model checkpoint to the specified path in Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe96ff59"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the save path and save the model and tokenizer to Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ec9d6bf",
        "outputId": "08092d73-12e0-4c74-b80f-69d36678f6b2"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to save the model checkpoint in Google Drive\n",
        "save_path = global_checkpoint_path\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model checkpoint and tokenizer saved to: {save_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model checkpoint and tokenizer saved to: /content/drive/MyDrive/llama3_8b_math_verifier_checkpoint_less_naive_8k5_steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feb0b9a5"
      },
      "source": [
        "### Load model from checkpoint\n",
        "\n",
        "#### Subtask:\n",
        "Load the model from the saved checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d984f7ec"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the model and tokenizer from the saved checkpoint path in Google Drive and prepare the model for inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "d9e6fa288ab74bd1b07b5cd3f09143c0",
            "baf3e4c3b0dc40aa87a958d6da7e4159",
            "0f951a9cc0b048978dc86d23b8cf736f",
            "529f751444a8402291d12c84faa3a550",
            "d1066e44454d42faac0ef8f7c639ed54",
            "229c1aca17ec43ca82c2d1961dc2d2d7",
            "806d639b44c4404493ba522db695ba5e",
            "af2c9c32d9c04dc481ca5070159d6bb9",
            "9af2491ac906478b8219c402c84b7eb2",
            "f7ceb61ac9d6485abed480d30f9c7c7f",
            "c83f43223c99411c84d8a3282c3a9bcc"
          ]
        },
        "id": "cc269188",
        "outputId": "3e94caf6-2ba3-47a0-f6b8-81d33d096e79"
      },
      "source": [
        "# Define the path where the model checkpoint was saved in Google Drive\n",
        "save_path = global_checkpoint_path\n",
        "\n",
        "# Load the model and tokenizer from the saved path\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Prepare the loaded model for faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(f\"Model and tokenizer loaded from: {save_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9e6fa288ab74bd1b07b5cd3f09143c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded from: /content/drive/MyDrive/llama3_8b_math_verifier_checkpoint_less_naive_8k5_steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32f497a5"
      },
      "source": [
        "## Step 8: Generate submission file\n",
        "\n",
        "### Subtask:\n",
        "Generate the submission CSV file using the loaded model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bc4e9ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate the submission CSV file by iterating through the test dataset, generating predictions using the loaded model, and saving the results to a pandas DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "185bd13d",
        "outputId": "8fc3fa7a-49c9-4974-e552-b8d5e971743a"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the official test set\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "predictions = []\n",
        "\n",
        "# Create the prompt template for inference (no answer included)\n",
        "inference_prompt = global_inference_prompt\n",
        "\n",
        "# A simple function to parse 'True' or 'False' from the model's raw output\n",
        "def parse_output(response_text):\n",
        "    # Find the text after \"Output:\"\n",
        "    output_part = response_text.split(\"Output:\\n\")[-1]\n",
        "    # Check if \"True\" is in that part, case-insensitively\n",
        "    if 'true' in output_part.lower():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Loop through the test dataset and generate a prediction for each example\n",
        "for example in tqdm(test_dataset):\n",
        "    question = example[\"question\"]\n",
        "    solution = example[\"solution\"]\n",
        "\n",
        "    # Format the prompt\n",
        "    prompt = inference_prompt.format(question, str(solution))\n",
        "    # Use the same data cleaning step that we used previously during training\n",
        "    prompt = clean_text(prompt)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the prediction\n",
        "    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
        "    response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Parse the prediction and add it to our list\n",
        "    prediction = parse_output(response_text)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'is_correct': predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
        "print(\"You can now download this file and submit it to the Kaggle competition.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [31:28<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Submission file 'submission.csv' created successfully!\n",
            "You can now download this file and submit it to the Kaggle competition.\n"
          ]
        }
      ]
    }
  ]
}