{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reproducer Notebook\n",
        "\n",
        "ECE-GY 7123 / CS-GY 6953 / Deep Learning - Fall '25 - Midterm Project\n",
        "\n",
        "**Team:** Spline Reticulator\n",
        "\n",
        "**Author/Member:** Thanh Do (qd2121@nyu.edu)\n",
        "\n",
        "**Purpose:** For regenerating validation accuracy and Kaggle response file for saved weight on Google Drive"
      ],
      "metadata": {
        "id": "jngXVlfmqT9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Necessary Libraries\n",
        "\n",
        "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
      ],
      "metadata": {
        "id": "h6H4hQVSqblY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xStnwtpOqK0e",
        "outputId": "dfeb1ccf-eff4-4750-c4d8-a36fe90d8d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: xformers==0.0.32.post2 in /usr/local/lib/python3.12/dist-packages (0.0.32.post2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (25.1.1)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.12/dist-packages (2025.10.13)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
            "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (1.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.13.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.10.12)\n",
            "Requirement already satisfied: transformers==4.56.2 in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2025.10.5)\n",
            "Requirement already satisfied: trl==0.22.2 in /usr/local/lib/python3.12/dist-packages (0.22.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining Model and Constants\n",
        "\n",
        "Change the `global_checkpoint_path` to the corresponding weights folder path in your Google Drive here. Please note that the prompts are not saved as part of the checkpoints, so you will have to remember it by yourself.\n",
        "\n"
      ],
      "metadata": {
        "id": "TkuYDaVuravN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "global_checkpoint_path = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint_naive\"\n",
        "\n",
        "max_seq_length = 2048  # 1024 is not enough for some questions\n",
        "dtype = None  # This will auto-detect the best data type for your GPU\n",
        "# We have a lot of VRAM on the A100 so this might save some quantization time\n",
        "load_in_4bit = False\n",
        "\n",
        "# Some constants\n",
        "generate_submission_file = True # Run on the actual test dataset & create CSV\n",
        "global_seed = 313337\n",
        "global_inference_prompt = \"\"\"You are an expert mathematician and a meticulous verifier.\n",
        "Your task is to evaluate a proposed solution to a math problem and determine if it's correct or not.\n",
        "Carefully read the Question and the Solution. Determine if the Solution is a correct reasoning process to solve the Question.\n",
        "Your response should be 'True' if the solution is correct, otherwise 'False'.\n",
        "Below is the Question and Solution.\n",
        "Question:\n",
        "{}\n",
        "Solution:\n",
        "{}\n",
        "Output:\n",
        "\"\"\"\n",
        "global_training_prompt = global_inference_prompt + \"{}\" # include the correct answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "5f2a5c6ea58748a59d88d8220abea6e8",
            "71e2cf046ba341d5adf6076675e44871",
            "a0cc0fcafee84ea7a2d3ed35595cb414",
            "c20c004fe6074e45ade01c92a45124b2",
            "8a373869b0e3441fbc60d4bf32a84661",
            "ea1aa5cd831745f88a0a8390b1391a4d",
            "063c914e80dc483b82664b8e12ecdd8b",
            "2f5632f3d61a4cb19a6b6bde79e6642c",
            "1129621868814f51977d4d3c1f4cb817",
            "d6324d73df39406a9d2f8e5bbd7fa4b0",
            "7b7229f6d4b9419cbf656e3c78e1c845"
          ]
        },
        "id": "URSw7qlhqlgB",
        "outputId": "afd44537-8864-4f26-be0c-d6d934c2b576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f2a5c6ea58748a59d88d8220abea6e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare the Dataset\n"
      ],
      "metadata": {
        "id": "sRznzEwL3W-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full training dataset\n",
        "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "\n",
        "# Shuffle the dataset for randomness and create our smaller splits\n",
        "shuffled_dataset = full_dataset.shuffle(seed=global_seed)\n",
        "n = len(full_dataset)\n",
        "# Takes the last 500 samples for internal validation\n",
        "n_validation = 500\n",
        "validation_dataset = shuffled_dataset.select(range(n - n_validation, n))"
      ],
      "metadata": {
        "id": "etaDwWGN3X7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightweight cleaner to round floats in text:"
      ],
      "metadata": {
        "id": "MZG3AsMZUXv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def round_floats_in_text(text: str, n_digits: int = 4) -> str:\n",
        "    \"\"\"\n",
        "    Finds all floating-point numbers in a string and rounds them\n",
        "    to a specified number of decimal places.\n",
        "    \"\"\"\n",
        "\n",
        "    # Matches an optional sign, digits, a decimal point, and more digits.\n",
        "    float_pattern = re.compile(r\"[-]?\\d+\\.\\d+\")\n",
        "\n",
        "    # m.group(0) is the matched text (e.g., \"0.666666667\")\n",
        "    def replacer(match):\n",
        "        # Convert the matched string to a float\n",
        "        number = float(match.group(0))\n",
        "        # Round it to 'n_digits'\n",
        "        rounded_number = round(number, n_digits)\n",
        "        # Convert it back to a string\n",
        "        return str(rounded_number)\n",
        "\n",
        "    return float_pattern.sub(replacer, text)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "  return round_floats_in_text(text)"
      ],
      "metadata": {
        "id": "d2_f4JgJUVww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b1e0a43"
      },
      "source": [
        "## Mount google drive\n",
        "\n",
        "### Subtask:\n",
        "Mount Google Drive to save the model checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8ab404"
      },
      "source": [
        "**Reasoning**:\n",
        "Mount Google Drive to save the model checkpoint.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e020e6b",
        "outputId": "9cab9400-757b-42d3-ab1e-22ca061a618a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feb0b9a5"
      },
      "source": [
        "## Load model from checkpoint\n",
        "\n",
        "### Subtask:\n",
        "Load the model from the saved checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d984f7ec"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the model and tokenizer from the saved checkpoint path in Google Drive and prepare the model for inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "e6430ed2f2d941e2b5f92d06d9b69e63",
            "5be36d4cb2884324b3a146360eca4806",
            "b7cf9e83c756495bae826215eb11ac08",
            "a772055cd82940d0baba5dc705565343",
            "e7ca901bd86a4cc4a8972191780d4dc0",
            "16b85863aace46bfb98d9b8aedbd09c4",
            "5430d34214d64603b4dc9d36a9bd6258",
            "5fe3ed3646054087afae41222fa741fb",
            "e7a6643e5aef491a8fb2830af505b269",
            "5961814ab66f4540875858dc02cebf2e",
            "ec49837e62ca4fde9d2f8c34a339add9",
            "7506e8242f6d42e3bc45442cdbc84c39",
            "1250636d99744791bdf2aec71d04498a",
            "53fd0f5c386d4b58bde0175ea8f39fca",
            "12825ea43114447e85679dcce37b65f6",
            "d93b5826dfaa41fd9e50d3971cbad346",
            "9f669e61122242b6b2a01b5685dd268e",
            "c982352ae7034f3db9566678bf3e40e1",
            "ba9e983249344742a5f5105753b60325",
            "4133bd4480df42d3bbecd06eb3733b89",
            "b9d14ff072e54ece87ea4b5e5472de58",
            "a451ef9567764b7e97beabe5497fa331"
          ]
        },
        "id": "cc269188",
        "outputId": "2e469e49-ee66-4733-a721-bc1a11ae6c4f"
      },
      "source": [
        "# Define the path where the model checkpoint was saved in Google Drive\n",
        "save_path = global_checkpoint_path\n",
        "\n",
        "# Load the model and tokenizer from the saved path\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Prepare the loaded model for faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(f\"Model and tokenizer loaded from: {save_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6430ed2f2d941e2b5f92d06d9b69e63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7506e8242f6d42e3bc45442cdbc84c39"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prompt template for inference (no answer included)\n",
        "inference_prompt = global_inference_prompt\n",
        "\n",
        "# Evaluate accuracy on our validation set\n",
        "num_samples = len(validation_dataset)\n",
        "count_correct = 0\n",
        "for i in range(num_samples):\n",
        "  example = validation_dataset[i]\n",
        "  question = example[\"question\"]\n",
        "  solution = example[\"solution\"]\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "      inference_prompt.format(question, str(solution))\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 8, use_cache = True)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  prediction: str = response[0].split(\"Output:\\n\")[1]\n",
        "  if prediction.startswith(str(example[\"is_correct\"])):\n",
        "    count_correct += 1\n",
        "print(\"Validation Accuracy =\", count_correct / num_samples)"
      ],
      "metadata": {
        "id": "Zih3QwJWyh1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32f497a5"
      },
      "source": [
        "## Generate submission file\n",
        "\n",
        "### Subtask:\n",
        "Generate the submission CSV file using the loaded model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bc4e9ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate the submission CSV file by iterating through the test dataset, generating predictions using the loaded model, and saving the results to a pandas DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "185bd13d"
      },
      "source": [
        "if generate_submission_file:\n",
        "\n",
        "  import pandas as pd\n",
        "  from tqdm import tqdm\n",
        "  from datasets import load_dataset\n",
        "\n",
        "  # Load the official test set\n",
        "  test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "  predictions = []\n",
        "\n",
        "  # Create the prompt template for inference (no answer included)\n",
        "  inference_prompt = global_inference_prompt\n",
        "\n",
        "  # A simple function to parse 'True' or 'False' from the model's raw output\n",
        "  def parse_output(response_text):\n",
        "      # Find the text after \"Output:\"\n",
        "      output_part = response_text.split(\"Output:\\n\")[-1]\n",
        "      # Check if \"True\" is in that part, case-insensitively\n",
        "      if 'true' in output_part.lower():\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "  # Loop through the test dataset and generate a prediction for each example\n",
        "  for example in tqdm(test_dataset):\n",
        "      question = example[\"question\"]\n",
        "      solution = example[\"solution\"]\n",
        "\n",
        "      # Format the prompt\n",
        "      prompt = inference_prompt.format(question, str(solution))\n",
        "      # Use the same data cleaning step that we used previously during training\n",
        "      prompt = clean_text(prompt)\n",
        "      inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "      # Generate the prediction\n",
        "      outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
        "      response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "      # Parse the prediction and add it to our list\n",
        "      prediction = parse_output(response_text)\n",
        "      predictions.append(prediction)\n",
        "\n",
        "  # Create the submission DataFrame\n",
        "  submission = pd.DataFrame({\n",
        "      'ID': range(len(predictions)),\n",
        "      'is_correct': predictions\n",
        "  })\n",
        "\n",
        "  # Save the DataFrame to a CSV file\n",
        "  # Add a suffix to denote which weight file we're using\n",
        "  suffix = global_checkpoint_path.split('/')[-1]\n",
        "  submission.to_csv(f'submission_{suffix}.csv', index=False)\n",
        "\n",
        "  # Calculate the submission hash for quick comparison\n",
        "  import hashlib\n",
        "  with open(f'submission_{suffix}.csv', 'rb') as f:\n",
        "      submission_hash = hashlib.sha1(f.read()).hexdigest()\n",
        "  print(f\"Submission SHA1 hash: {submission_hash}\")\n",
        "\n",
        "  print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
        "  print(\"You can now download this file and submit it to the Kaggle competition.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
