{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " <h1>\n",
        "Welcome to the Math Question Answer Verification Competition! ğŸš€\n",
        "\n",
        "The goal is to fine-tune a Llama-3-8B model to predict if a given solution to a math problem is correct or not. Your model should output True if the solution is correct, and False otherwise.\n",
        "\n",
        "This notebook is a starter guide designed to get you up and running quickly. We'll walk through a simplified training process using a small subset of the data (5,000 examples) and lightweight parameters. The main goal here is to understand the complete workflow, from loading data to generating a submission file, not to achieve a top score.\n",
        "\n",
        "Good luck, and have fun! ğŸ‰"
      ],
      "metadata": {
        "id": "jngXVlfmqT9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Install Necessary Libraries**\n",
        "\n",
        "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
      ],
      "metadata": {
        "id": "h6H4hQVSqblY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xStnwtpOqK0e",
        "outputId": "cbf8e185-2875-4e17-808f-cd88a1210489"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Collecting xformers==0.0.32.post2\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Collecting trl\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.11.1-py3-none-any.whl.metadata (32 kB)\n",
            "Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.11.1-py3-none-any.whl (276 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m276.7/276.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 trl-0.24.0 unsloth_zoo-2025.11.1 xformers-0.0.32.post2\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
            "Collecting datasets<4.0.0,>=3.4.1\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (1.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.13.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m775.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.11.1 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.11.1 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.11.1 requires torchao>=0.13.0, but you have torchao 0.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0\n",
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.11.1-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.11.1-py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.8/348.8 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.11.1\n",
            "Collecting transformers==4.56.2\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2025.10.5)\n",
            "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.11.1 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.11.1 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.11.1 requires tyro, which is not installed.\n",
            "unsloth 2025.11.1 requires trl!=0.19.0,<=0.23.0,>=0.18.2, but you have trl 0.24.0 which is incompatible.\n",
            "unsloth-zoo 2025.11.1 requires torchao>=0.13.0, but you have torchao 0.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed transformers-4.56.2\n",
            "Collecting trl==0.22.2\n",
            "  Downloading trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading trl-0.22.2-py3-none-any.whl (544 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.24.0\n",
            "    Uninstalling trl-0.24.0:\n",
            "      Successfully uninstalled trl-0.24.0\n",
            "Successfully installed trl-0.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Load the Model and Tokenizer**\n",
        "\n",
        "Next, we'll load the Llama-3-8B model, which is the only model permitted for this competition. We'll use Unsloth's FastLanguageModel to handle this efficiently.\n",
        "\n",
        "A key technique we'll use is 4-bit quantization (load_in_4bit = True). Think of this as compressing the model's knowledge into a much smaller file size. This significantly reduces the amount of GPU memory required, allowing us to fine-tune this large model even on a free platform like Google Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "TkuYDaVuravN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1024  # Choose any sequence length\n",
        "dtype = None  # This will auto-detect the best data type for your GPU\n",
        "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
        "\n",
        "# Some constants\n",
        "global_seed = 42\n",
        "global_inference_prompt = \"\"\"You are a great mathematician and you are tasked with finding if a solution to a given maths question is correct or not. Your response should be 'True' if the solution is correct, otherwise 'False'. Below is the Question and Solution.\n",
        "Question:\n",
        "{}\n",
        "Solution:\n",
        "{}\n",
        "Output:\n",
        "\"\"\"\n",
        "global_training_prompt = global_inference_prompt + \"{}\" # include the correct answer\n",
        "global_checkpoint_path = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint_baseline\"\n",
        "global_max_steps = 60 # Same as in the starter notebook\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "# Note: We use the base model, not a 4-bit pre-quantized one,\n",
        "# to ensure we start from the official weights.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\", # Competition-approved model\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "792a96d0ac3e455da34f5dc747b196d2",
            "9ad93da3e49545a8bdc9f8a82b65f103",
            "fd8ceb18bb804c778ac1b5e6547d456e",
            "18b27e2f59c24700a11347e4e50267a2",
            "ad51124243404933be2e7260ae01e290",
            "59b857841c934e5cb59f896f3af45cf2",
            "bae1c3fae66847b18e876fe3396a3f8f",
            "ae9b1417fac944ed84d0f1cc28bbba24",
            "c6f3706b9eb14994af388f8f8c237ff3",
            "b8196b59e0674f97b0d82ef929f37aa3",
            "38d193dc7aa148c282687c48ba9a2ce7",
            "eb3e601006f4497a9ef8d2d253c67025",
            "63408e32e8e0477187fa2bb5617fd6bd",
            "8f1803ecc88e4ab5a60aadba267b6fac",
            "5093941cb45046c487267f96124cec13",
            "5d1f3f6106b94b6082ac3daa65529854",
            "b601f2e698e64975a911d768086b3047",
            "3fc659a26ebe456296780b8f8ced472e",
            "32cfeda944ae404595d81c94d20d7802",
            "f30b73ffe2e347c098b1bdb09dd91045",
            "8118748dc4634c8c8b5b1aa1d412f990",
            "d597d50359324397ab4bc17e08fd7181",
            "b6c69348765a481aa8c37c50d7efb8de",
            "ca49e836a179467fbf6b28eef808f41b",
            "846a7d9a15fe4825a712e1ff56d0b08d",
            "b8bd93a82bf1422b9ff027128a48b044",
            "6df2c05ea94049f581b93f77f450b00d",
            "2289811230994156a4ef538793045182",
            "557af5e50ddd43d1a208191b20d79d60",
            "a968fc43ec9947bb8ef3ed8c18406c94",
            "4f22d762f80d4829b0abef67259c2def",
            "5a2c9becc05f471785c25ce8cbe0f4bd",
            "09f9feca9ad94bc981b94f46619b9afb",
            "7ec6af44754448b4902d2804e7a73598",
            "81085c4c9e5b43ac94832c32d784c159",
            "c10239511a3a4a79be801f62453d4007",
            "0fdd16bffabd45cdb14a339b063b00e8",
            "96a0699bb7314cf287d2baaa6fe40f9a",
            "20d131f1af1643f1a51cbd0a108384b1",
            "1e280c6f2db14108b57e72367f1549bc",
            "561ef2420d6f4ee0888cf10b675a8f1b",
            "0a0f61c147604a7f8645e3edbf29ded5",
            "977ae44e441d47f08f49098132eda960",
            "4573484d84a04bdfb5ded16405afe498",
            "7b8f306dbc8b402bb4642b79994b730d",
            "5f3347ceabad474c8d5d89e75c28d556",
            "e258c333a3f24ad6b0ee4f874ba3cce6",
            "3db2452832c440e7aba1a36268c4631d",
            "6b4632f4bf144c398c536de9acb4d407",
            "776027b3becf4ea7847dee4573404126",
            "336ac0d357924ce2a9d4942b3ed2ed0b",
            "5530719c59f24021818429ebf10ea482",
            "04042e067f154da5bedd1bc1413ff064",
            "9f444028d0194d948b612f85d613e71f",
            "33a52bef897a4939a1f62d056b536519"
          ]
        },
        "id": "URSw7qlhqlgB",
        "outputId": "92ea5ad2-c648-44e0-d330-0044d311b9cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.1: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "792a96d0ac3e455da34f5dc747b196d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb3e601006f4497a9ef8d2d253c67025"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6c69348765a481aa8c37c50d7efb8de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ec6af44754448b4902d2804e7a73598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b8f306dbc8b402bb4642b79994b730d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Prepare the Dataset**\n",
        "\n",
        "This is a crucial step where we format our data into a structure the model can learn from. The process involves three parts:\n",
        "\n",
        "1.  **Loading**: We'll load the official competition dataset from Hugging Face.\n",
        "2.  **Splitting**: The full dataset is massive. For this starter notebook, we'll create a much smaller, more manageable version to speed things up: **5,000 samples for training** and **500 for validation**.\n",
        "3.  **Prompting**: We will format each data sample into a clear instructional prompt. This helps the model understand its role as a mathematician verifying a solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "sRznzEwL3W-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full training dataset\n",
        "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "\n",
        "# Shuffle the dataset for randomness and create our smaller splits\n",
        "# Note that the seed here is changed to align the same validation dataset\n",
        "shuffled_dataset = full_dataset.shuffle(seed=313337)\n",
        "n = len(full_dataset)\n",
        "# Takes the last 500 samples for internal validation\n",
        "n_validation = 500\n",
        "train_dataset = shuffled_dataset.select(range(n - n_validation))\n",
        "validation_dataset = shuffled_dataset.select(range(n - n_validation, n))"
      ],
      "metadata": {
        "id": "etaDwWGN3X7C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "734ff50ee7784a7e81e5e418e9095a5b",
            "0e4bc67fdcb6456f8a54258d53941c1b",
            "b2ac267423324b7cbcb45ec981699095",
            "e21672ee2fa8481ea810b6f3c8cf16b3",
            "b4df00c6929c400b9325b302b354592f",
            "3d747ecbea3247bea6d7d507db2756ae",
            "6c33f530e92c473e93cf4b4db9c9deee",
            "2e933050a5854b14a2138390f038343c",
            "02c83cb34ca6485f8c09d727eb5b3dbc",
            "0b46998eab914ec48ffac1b450ba141b",
            "320ef0527eed491b9a2fa6b2e5da4e54",
            "1dd10f27b9644f8195f1514e69245144",
            "de792444e7c84decab51932ea679ea3e",
            "45229c9fa281445b90f643184a1271c6",
            "823a0ae4d8434a708405e7324bf28e8a",
            "486c40b4cd8740df8c04d853a64108f4",
            "c8aa63cd22904f0e95bb7def3b0f8640",
            "36273b1aa3454bb7923afda0cd212c66",
            "e37cb362102c4964b8f946469fdff102",
            "3cf3611d701446b899de5e4a2a9e753d",
            "cb405ee42f174f61bddecb02321be84d",
            "6689be3a89ec4ea9ad4200e86d18c474",
            "69928486a221464eb39db8de43a4c552",
            "c3181551184a4658938685f7254ca76a",
            "b9b091b154be4a9492600786bc1f9b12",
            "9c12b5031d724cf5be787b1cf7b2b853",
            "9d3d34f091564b06ba56db45d4b82c04",
            "c1dc4098cc2f4deca75a411c8f99be75",
            "aca58416551449f7a46b81f856bddf35",
            "210bfc6e11e3433386cde99f992dee50",
            "57b9ce9aff024a57a11b70eedaa1781d",
            "e3624be33ce741fd886174ea069daf83",
            "d9c38d20232948598eb3676cfacd162b",
            "71c407ea43514e82b78c6c635e22e417",
            "4eded59ff7ab4777a691b1778b714016",
            "0582d6e2b44446eba7b279dc0fa88a74",
            "83bb7bb8aa5c447cac2964237e37155a",
            "370a9710a43e4ca39d441cc2088ac50c",
            "54ba8579875b44409cd0134481d0d5eb",
            "5f3cbf0803a94d47a8284e2e5a2385c2",
            "034ba0707456448a9536498ec0bafaa5",
            "fabfa83d5c004b86b5ffcaea6ece47ce",
            "4edbd24d80d443cb8dbb04aea2f3d54f",
            "3a277c20fe0d4537a2dd907a578b20c3",
            "2ad65f2dd7e544c5b4583441c1722686",
            "6a0d793bb8864b4f8146ff9368fe67c3",
            "c2ef54b509b94fc38d9981c1ff2fad50",
            "8dc2f0137cd24eb2a49a48e9ee7ecb16",
            "bede911c048a49319d3ee77ce3d448dd",
            "3a4d03588a39481fb1e003a71c471927",
            "73f4ddb2e0d342cd8daf34d49133061a",
            "e9110bd5845e47ba9c8264f1e7ecc03b",
            "de2c4fab8c25438d8940f471408aec7b",
            "1f7b621fdd394120a391fea03d3e647c",
            "7c54a517afec4bcd8be79bf5f67f5a0a",
            "b1a50ff66a7a4ff680f5bc1c5f834440",
            "1aaeb0d3932643b58577aee9766371ea",
            "66217229930747109034ae149c1c1fd5",
            "186acf469be24957bba0b834bc2aec53",
            "5d57bd319ce945c9bf80cea46ff520b4",
            "a62675472c334ef5960d4df2ea6270af",
            "8f703353cf944a0fa8c05799b76cefac",
            "0fb578302cba43e395e9b993531fc6f6",
            "6adcaa442edc49d39bc32d071dcfa03a",
            "c2e45d259c5e4c6c8068122059e620b0",
            "b1cce49bac3e42b4b53ba1e0d09d8bd3"
          ]
        },
        "outputId": "b27eaeb8-e21f-4911-f977-d3d7545022a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "734ff50ee7784a7e81e5e418e9095a5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dd10f27b9644f8195f1514e69245144"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00001-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69928486a221464eb39db8de43a4c552"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/3.65M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71c407ea43514e82b78c6c635e22e417"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ad65f2dd7e544c5b4583441c1722686"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1a50ff66a7a4ff680f5bc1c5f834440"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare training:"
      ],
      "metadata": {
        "id": "uJCEJoegUghM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The instructional prompt template for training\n",
        "training_prompt = global_training_prompt\n",
        "\n",
        "# We must add an End Of Sequence (EOS) token to tell the model when a completion is finished.\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# This function formats our data samples into the prompt template.\n",
        "def formatting_prompts_func(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    solutions = examples[\"solution\"]\n",
        "    outputs = examples[\"is_correct\"]\n",
        "    texts = []\n",
        "    for question, solution, output in zip(questions, solutions, outputs):\n",
        "        # Format the prompt and add the EOS token\n",
        "        text = training_prompt.format(question, str(solution), str(output))\n",
        "        text += EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# Apply the formatting function to our training dataset\n",
        "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "i5cL3djv3bRy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "92c5a7a7948a44b4a44fa994dbfc89ac",
            "be8c2ad021504e2da67245f17ccc4d18",
            "2ee4f16a1c7142eb8a6da3362a75e855",
            "efdf95b49a504134af2edaf5f61637d3",
            "cbfab2af026d4e25b4135c84163a44d5",
            "26eeb706107448c6a02bb2f7ff7dde5f",
            "0ebae5ee750141c4bd6305116de8776b",
            "2c6c31ee42dd406bb89f96e6a590bbab",
            "451f06282e7e4c68a92353b3476ff7e8",
            "e2b10d4c59d44b4391a7f0aca2dedb2c",
            "59908d3cfca248e78b05bac0fc57f098"
          ]
        },
        "outputId": "5e260195-9161-4068-dca2-868cc729cfac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/999500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92c5a7a7948a44b4a44fa994dbfc89ac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Configure LoRA and Set Up the Trainer**\n",
        "\n",
        "### **LoRA Configuration**\n",
        "\n",
        "Instead of training the entire model (which has billions of parameters), we'll use a technique called **Lo**w-**R**ank **A**daptation (LoRA). ğŸ›ï¸\n",
        "\n",
        "Think of it like this: rather than rewriting an entire textbook, we're just adding small, efficient \"sticky notes\" (the LoRA adapters) to update the model's knowledge. This is much faster and requires significantly less memory. We'll use a small **rank** (`r = 8`) to keep the training process light and quick for this starter notebook.\n"
      ],
      "metadata": {
        "id": "K8Fs1qmn37-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 1, # A small rank for lighter training\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 2, # A common practice is to set alpha = 2 * r\n",
        "    lora_dropout = 0, # Unsloth will complain about perf if not zero\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = global_seed,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEaRjozB3tz8",
        "outputId": "735b8cd3-d781-442e-830d-cf3bf39b8a9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **SFTTrainer Setup**\n",
        "\n",
        "Now we'll set up the `SFTTrainer` (Supervised Fine-tuning Trainer). This is the main tool from the `trl` library that will handle the entire training loop for us. We'll give it our model, tokenizer, dataset, and a set of training instructions, such as the batch size and number of epochs.\n",
        "\n",
        "We will train for just **one epoch** (a single pass over our 5,000-sample dataset) to keep this demonstration fast."
      ],
      "metadata": {
        "id": "HCHdotc14DgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_train_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = TrainingArguments(\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = global_max_steps,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = global_seed,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "yVZHQ4y74BCG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "625aba7e409545a1ba3bdf47cba5218b",
            "a0f89d6922aa4749b75b46b314dc15bc",
            "6387c8b4ce3e4649a3754ce721d70e86",
            "295300de14714db3b1b3dd4163775f0e",
            "03dfddd236914940ab1755aa6def217b",
            "720f44609dd04beebea103e44634554a",
            "28b1490b1d2b40c2b822020c2207555a",
            "93dd69b7953b430bb5549802b2d2cfef",
            "69bb0c7496174c41a7a70131452fa06d",
            "e9b466b57ce24926ba391df05b4ff958",
            "ac89a6e2eac1477e88858749e7d348e1"
          ]
        },
        "outputId": "2e8dc79e-2218-4c06-8ace-77f7ba64e84c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/999500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "625aba7e409545a1ba3bdf47cba5218b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Start Training\\!**\n",
        "\n",
        "Now, we'll call the `train()` function on our `trainer` object. This will kick off the fine-tuning process. Based on our settings, this will run for one full epoch over our 5,000 examples.\n",
        "\n",
        "Grab a coffee, as this will take a few minutes\\! â˜•\n"
      ],
      "metadata": {
        "id": "GTHBzKeM4zF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "YVrNhZ4y4zsK",
        "outputId": "f374adaf-c26b-4a9d-d610-01f7a8c08f4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 999,500 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 2,621,440 of 8,032,882,688 (0.03% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 04:05, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.471400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.085400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.855600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.833800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.801900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.786700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60, training_loss=0.9724661111831665, metrics={'train_runtime': 255.8454, 'train_samples_per_second': 1.876, 'train_steps_per_second': 0.235, 'total_flos': 7669228625068032.0, 'train_loss': 0.9724661111831665, 'epoch': 0.00048024012006003})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Step 6: Inference and Evaluation**\n",
        "\n",
        "Now that our model is trained, we need to test it on our validation set. We'll use a slightly different prompt for inferenceâ€”one where we leave the `Output:` section blank for the model to complete.\n",
        "\n",
        "Let's test it on a single example from our validation set to see what it predicts."
      ],
      "metadata": {
        "id": "8LfJfk5gIyIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Create the prompt template for inference (no answer included)\n",
        "inference_prompt = global_inference_prompt\n",
        "\n",
        "# Evaluate accuracy vs our validation\n",
        "num_samples = len(validation_dataset)\n",
        "count_correct = 0\n",
        "for i in range(num_samples):\n",
        "  example = validation_dataset[i]\n",
        "  question = example[\"question\"]\n",
        "  solution = example[\"solution\"]\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "      inference_prompt.format(question, str(solution))\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 8, use_cache = True)\n",
        "  response = tokenizer.batch_decode(outputs)\n",
        "  prediction: str = response[0].split(\"Output:\\n\")[1]\n",
        "  if prediction.startswith(str(example[\"is_correct\"])):\n",
        "    count_correct += 1\n",
        "print(\"Validation Accuracy =\", count_correct / num_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agvQR_Ku5wWY",
        "outputId": "20eb8599-eeba-4045-f9e3-54b34a18954c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy = 0.61\n"
          ]
        }
      ]
    }
  ]
}
